{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Theory Deep Learning Experiments\n",
    "\n",
    "This notebook demonstrates training neural networks to play different game theory setups and analyze equilibriums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from game_theory import *\n",
    "from models import *\n",
    "from experiments import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Game Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prisoner's Dilemma payoffs:\n",
      "Player 1: [[3 0]\n",
      " [5 1]]\n",
      "Player 2: [[3 5]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Create different game instances\n",
    "pd_game = prisoners_dilemma()\n",
    "coord_game = coordination_game()\n",
    "mp_game = matching_pennies()\n",
    "\n",
    "print(\"Prisoner's Dilemma payoffs:\")\n",
    "print(\"Player 1:\", pd_game.payoff_p1)\n",
    "print(\"Player 2:\", pd_game.payoff_p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train MLP Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Prisoner's Dilemma...\n",
      "Episode 0: P1=2.215, P2=2.332\n",
      "Episode 100: P1=3.000, P2=3.000\n",
      "Episode 200: P1=3.000, P2=3.000\n",
      "Episode 300: P1=3.000, P2=3.000\n",
      "Episode 400: P1=3.000, P2=3.000\n",
      "Episode 500: P1=3.000, P2=3.000\n",
      "Episode 600: P1=3.000, P2=3.000\n",
      "Episode 700: P1=3.000, P2=3.000\n",
      "Episode 800: P1=3.000, P2=3.000\n",
      "Episode 900: P1=3.000, P2=3.000\n",
      "\n",
      "Final Results:\n",
      "Player 1 strategy: [1.0000000e+00 8.9574576e-10]\n",
      "Player 2 strategy: [1.0000000e+00 1.3644568e-08]\n",
      "Is Nash Equilibrium: False\n"
     ]
    }
   ],
   "source": [
    "# Train on Prisoner's Dilemma\n",
    "print(\"Training on Prisoner's Dilemma...\")\n",
    "p1, p2 = train_players(pd_game, n_episodes=1000)\n",
    "\n",
    "# Evaluate final strategies\n",
    "results = evaluate_equilibrium(pd_game, p1, p2)\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Player 1 strategy: {results['strategy_p1']}\")\n",
    "print(f\"Player 2 strategy: {results['strategy_p2']}\")\n",
    "print(f\"Is Nash Equilibrium: {results['is_nash']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Матричные игры с RL агентами\n",
    "\n",
    "Создаем произвольные матричные игры и обучаем агентов методами RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица выплат игрока 1:\n",
      "[[ 5  4  3]\n",
      " [ 4  3  2]\n",
      " [ 3  2 10]]\n",
      "\n",
      "Матрица выплат игрока 2:\n",
      "[[ 4  3  2]\n",
      " [ 3  2  3]\n",
      " [ 2  3 10]]\n"
     ]
    }
   ],
   "source": [
    "from models import RLAgent, train_rl_agents\n",
    "from game_theory import MatrixGame\n",
    "\n",
    "# Создаем матричную игру 3x3\n",
    "payoff_p1 = np.array([\n",
    "    [5, 4, 3],\n",
    "    [4, 3, 2],\n",
    "    [3, 2, 10]\n",
    "])\n",
    "\n",
    "payoff_p2 = np.array([\n",
    "    [4, 3, 2],\n",
    "    [3, 2, 3],\n",
    "    [2, 3, 10]\n",
    "])\n",
    "\n",
    "matrix_game = MatrixGame(payoff_p1, payoff_p2)\n",
    "\n",
    "print(\"Матрица выплат игрока 1:\")\n",
    "print(payoff_p1)\n",
    "print(\"\\nМатрица выплат игрока 2:\")\n",
    "print(payoff_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение RL агентов...\n",
      "Episode 0: Avg rewards P1=3.000, P2=2.000\n",
      "Episode 1000: Avg rewards P1=9.090, P2=9.110\n",
      "Episode 2000: Avg rewards P1=9.000, P2=9.050\n",
      "\n",
      "Финальные стратегии:\n",
      "Игрок 1: [0.00254804 0.00153028 0.9959216 ]\n",
      "Игрок 2: [0.0188419  0.0352649  0.94589317]\n",
      "\n",
      "Ожидаемые выплаты: P1=9.558, P2=9.573\n"
     ]
    }
   ],
   "source": [
    "# Обучаем RL агентов с разными стратегиями exploration\n",
    "print(\"Обучение RL агентов...\")\n",
    "# Агент 1 без exploration, агент 2 с exploration\n",
    "agent1, agent2, history = train_rl_agents(matrix_game, n_episodes=3000, epsilon1=0.0, epsilon2=0.2)\n",
    "\n",
    "# Получаем финальные стратегии\n",
    "final_probs_p1 = agent1.get_action_probs()\n",
    "final_probs_p2 = agent2.get_action_probs()\n",
    "\n",
    "print(f\"\\nФинальные стратегии:\")\n",
    "print(f\"Игрок 1: {final_probs_p1}\")\n",
    "print(f\"Игрок 2: {final_probs_p2}\")\n",
    "\n",
    "# Ожидаемые выплаты\n",
    "exp_payoff1, exp_payoff2 = matrix_game.get_expected_payoffs(final_probs_p1, final_probs_p2)\n",
    "print(f\"\\nОжидаемые выплаты: P1={exp_payoff1:.3f}, P2={exp_payoff2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Эксперимент с разными стратегиями exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Стратегия 1: eps1=0.0, eps2=0.0\n",
      "Episode 0: Avg rewards P1=4.000, P2=3.000\n",
      "Episode 1000: Avg rewards P1=9.720, P2=9.700\n",
      "Выплаты: P1=9.937, P2=9.937, Сумма=19.874\n",
      "\n",
      "Стратегия 2: eps1=0.0, eps2=0.2\n",
      "Episode 0: Avg rewards P1=2.000, P2=3.000\n",
      "Episode 1000: Avg rewards P1=8.790, P2=8.810\n",
      "Выплаты: P1=9.609, P2=9.619, Сумма=19.229\n",
      "\n",
      "Стратегия 3: eps1=0.2, eps2=0.0\n",
      "Episode 0: Avg rewards P1=5.000, P2=4.000\n",
      "Episode 1000: Avg rewards P1=8.270, P2=8.130\n",
      "Выплаты: P1=9.654, P2=9.642, Сумма=19.296\n",
      "\n",
      "Стратегия 4: eps1=0.1, eps2=0.1\n",
      "Episode 0: Avg rewards P1=3.000, P2=2.000\n",
      "Episode 1000: Avg rewards P1=8.720, P2=8.750\n",
      "Выплаты: P1=9.630, P2=9.633, Сумма=19.263\n",
      "\n",
      "Лучшая стратегия: eps1=0.0, eps2=0.0\n",
      "Суммарная выплата: 19.874\n"
     ]
    }
   ],
   "source": [
    "# Сравниваем разные комбинации exploration\n",
    "strategies = [\n",
    "    (0.0, 0.0),  # Оба без exploration\n",
    "    (0.0, 0.2),  # Только второй с exploration\n",
    "    (0.2, 0.0),  # Только первый с exploration\n",
    "    (0.1, 0.1)   # Оба с exploration\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, (eps1, eps2) in enumerate(strategies):\n",
    "    print(f\"\\nСтратегия {i+1}: eps1={eps1}, eps2={eps2}\")\n",
    "    \n",
    "    agent1, agent2, _ = train_rl_agents(matrix_game, n_episodes=2000, \n",
    "                                        epsilon1=eps1, epsilon2=eps2)\n",
    "    \n",
    "    probs1 = agent1.get_action_probs()\n",
    "    probs2 = agent2.get_action_probs()\n",
    "    \n",
    "    exp_p1, exp_p2 = matrix_game.get_expected_payoffs(probs1, probs2)\n",
    "    \n",
    "    results[(eps1, eps2)] = {\n",
    "        'probs1': probs1,\n",
    "        'probs2': probs2,\n",
    "        'payoffs': (exp_p1, exp_p2),\n",
    "        'total': exp_p1 + exp_p2\n",
    "    }\n",
    "    \n",
    "    print(f\"Выплаты: P1={exp_p1:.3f}, P2={exp_p2:.3f}, Сумма={exp_p1+exp_p2:.3f}\")\n",
    "\n",
    "# Находим лучшую стратегию\n",
    "best_strategy = max(results.items(), key=lambda x: x[1]['total'])\n",
    "print(f\"\\nЛучшая стратегия: eps1={best_strategy[0][0]}, eps2={best_strategy[0][1]}\")\n",
    "print(f\"Суммарная выплата: {best_strategy[1]['total']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TheoryGamesDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
